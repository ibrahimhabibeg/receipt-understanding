{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gk2U3hsS9DIN"
   },
   "source": [
    "# Preprocessing Data to Build Recipt Understanding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K9zy20lW9DIN"
   },
   "source": [
    "In this notebook we will focus on preprocessing data to build the recipt understanding model. We will finetune Microsoft's open-source [LayoutLMv3 model](https://arxiv.org/abs/2204.08387) on the [Scanned Receipts OCR and Information Extraction (SROIE) dataset](https://paperswithcode.com/dataset/sroie). Our goal is to create train and test datasets that ready to be consumed by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nkbd2lw9DIO"
   },
   "source": [
    "## Data Loading Source\n",
    "The SROIE dataset was released in the [Robust Reading Competetion](https://rrc.cvc.uab.es/?ch=13). The dataset is shared with us as a Google Drive link containing all files. To help simplify the dataset loading process, we will use the dataset published on Kaggle which can be found [here](https://www.kaggle.com/datasets/urbikn/sroie-datasetv2).\n",
    "\n",
    "There are other datasets avillable on huggingface and kaggle that are also copies of the original SROIE dataset. I will be using this particular dataset as it contains the files in the same format as the one they released on Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6klStKZ9DIO"
   },
   "source": [
    "Before we load the dataset, we will need to download some neccessary packages and libraries.\n",
    "\n",
    "*Note: The downloaded packages are the ones not available on Google Colab by default. Downloading them is added here to make it simple and easy to quickly try the code in colab. If you are running this notebook locally, ensure that you're running the notebook in a dedicated conda environment or a virtual environment so that you don't pollute your base environment. If the notebook fails to run locally, make sure you have the other dependencies installed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wZs4rskJ9DIP"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! pip install datasets\n",
    "! pip install transformers\n",
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F7zAhC7F9DIP"
   },
   "source": [
    "Now we are going to use the official Kaggle API to download the dataset in the data/unprocessed folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9zR5OciE9DIP"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! kaggle datasets download -d urbikn/sroie-datasetv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FXP91bRS9DIQ"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "! mkdir -p data/unprocessed\n",
    "! unzip -o sroie-datasetv2.zip -d data/unprocessed\n",
    "! rm sroie-datasetv2.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mrnudLwJ9DIQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(626, 347)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "len(os.listdir('data/unprocessed/SROIE2019/train/img')), len(os.listdir('data/unprocessed/SROIE2019/test/img'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJuiyKJx9DIQ"
   },
   "source": [
    "The train dataset contains 626 images and the test dataset contains 347 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6fp2rJZ9DIQ"
   },
   "source": [
    "## Initial Data Structure\n",
    "\n",
    "The downloaded dataset contains two major folders:\n",
    "1. **train**: This folder contains the training data.\n",
    "2. **test**: This folder contains the test data.\n",
    "\n",
    "Each of these folders contains the following subfolders:\n",
    "1. **img**: This folder containing the images of the receipts.\n",
    "2. **box**: This folder containing the bounding box information of the text in the images and the text inside the bounding boxes. The file is a text file with the same name as the image file but with a .txt extension. The format of the file is as follows:\n",
    "    ```\n",
    "    x1, y1, x2, y2, x3, y3, x4, y4, text\n",
    "    ```\n",
    "    where (x1, y1), (x2, y2), (x3, y3), and (x4, y4) are the coordinates of the bounding box and text is the text inside the bounding box.\n",
    "3. **entities**: This folder contains the ground truth information of the entities in the receipts. The file is a text file with the same name as the image file but with a .txt extension. The file contains json data with the following format:\n",
    "    ```json\n",
    "    {\n",
    "        \"company\": \"COMPANY_NAME\",\n",
    "        \"date\": \"DATE\",\n",
    "        \"address\": \"ADDRESS\",\n",
    "        \"total\": \"TOTAL\",\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aqPArRn9DIQ"
   },
   "source": [
    "## Data Preprocessing Steps\n",
    "We will divide the data preprocessing steps into two major parts:\n",
    "1. **Data Collection**: In this step we will gather all the information spread across the three subfolders and create a single dataframe with the following columns:\n",
    "    - `image_path`: The path to the image file.\n",
    "    - `bboxes`: A list of bounding boxes in the image. Each bounding box is a list of 4 coordinates (x1, y1, x2, y2) where (x1, y1) is the top-left corner and (x2, y2) is the bottom-right corner of the bounding box. The coordinates are normalized between 0 and 1000 as required by the LayoutLMv3 model.\n",
    "    - `words`: A list of words in the image corresponding to the bounding boxes.\n",
    "    - `ner_tags`: A list of named entity recognition tags for each word in the image. The tags will encoded as numbers. The possible tags are:\n",
    "        - **O**: The word is not part of any named entity. (Other)\n",
    "        - **B-COMPANY**: The word is the company name.\n",
    "        - **B-DATE**: The word is the date.\n",
    "        - **B-ADDRESS**: The word is the address.\n",
    "        - **B-TOTAL**: The word is the total.\n",
    "2. **Data Processing**: The goal of the step is to create a huggingface dataset that can be consumed by the LayoutLMv3 model. The steps involved in this process are:\n",
    "    - **Reading Images**: We will read the images and resize them to a fixed size.\n",
    "    - **Tokenization**: We will tokenize the words in the images using the LayoutLMv3 tokenizer.\n",
    "    - **Adding Padding**: We will add padding to the tokenized words to make them of the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZeX18u4l9DIR"
   },
   "source": [
    "## Data Collection\n",
    "\n",
    "We will start by creating a list of all possible ner tags and then create a dictionary to map the tags to their corresponding indices and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "YYHMD6pv9DIR"
   },
   "outputs": [],
   "source": [
    "labels_list = ['O', 'B-COMPANY', 'B-DATE', 'B-ADDRESS', 'B-TOTAL']\n",
    "ids2labels = {k: v for k, v in enumerate(labels_list)}\n",
    "labels2ids = {v: k for k, v in enumerate(labels_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest challenge in preprocessing the data is to map each bounding box to the corresponding label. The text provided in the entities file is not always the same as the text in the bounding boxes. Thus, we will use the following approach to achieve this:\n",
    "1. If SequenceMatcher ratio between the bounding box text and an entity is greater than 0.8, we will assign the corresponding tag to the bounding box text.\n",
    "2. If a word in bounding box includes the date or total, we will assign the corresponding tag to the word. This is because the date and total are usually written in the same line with other words. For example, the total is usually written as \"Total: 100.00\" or \"Total 100.00\" and the date is usually written as \"Date: 2022-01-01\" or \"Date 2022-01-01\".\n",
    "3. If a word in bounding box is a part of the company name or address, we will assign the corresponding tag to the word. This is because the company name and address are usually written in multiple lines and the bounding boxes may not cover the entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "def get_word_tag(word, entities):\n",
    "  for entity in entities:\n",
    "    if SequenceMatcher(None, word.lower(), entities[entity].lower()).ratio() >= 0.8:\n",
    "      return labels2ids[f'B-{entity}']\n",
    "    elif entity in ['ADDRESS', 'COMPANY'] and word.lower() in entities[entity].lower():\n",
    "      return labels2ids[f'B-{entity}']\n",
    "    elif entity in ['DATE', 'TOTAL'] and entities[entity].lower() in word.lower():\n",
    "      return labels2ids[f'B-{entity}']\n",
    "  return labels2ids['O']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line in the box file contains the the bbox coordinates and the text in it seperated by a comma. We will create helper functions to parse the line and extract the bbox coordinates and the text. This function will also normalize the bbox coordinates between 0 and 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalized_bbox(line, img_width, img_height):\n",
    "  line = line.strip().split(',')\n",
    "  x1, y1, x2, y2 = int(line[0]), int(line[1]), int(line[6]), int(line[7])\n",
    "  return [x1 / img_width * 1000, y1 / img_height * 1000, x2 / img_width * 1000, y2 / img_height * 1000]\n",
    "\n",
    "def get_word(line):\n",
    "  line = line.strip().split(',')\n",
    "  return ','.join(line[8:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entities file may not contain all the entities in the image. Thus, we will create a helper function to fill in the missing entities with empty strings. This will allow us to easily access all entities without having to check if the entity exists in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_entities_dict(entities_file_path):\n",
    "  entities_file = open(entities_file_path, 'r')\n",
    "  entities = json.load(entities_file)\n",
    "  entities_file.close()\n",
    "  res_dict ={\n",
    "    'COMPANY': entities.get('company', ''),\n",
    "    'DATE': entities.get('date', ''),\n",
    "    'ADDRESS': entities.get('address', ''),\n",
    "    'TOTAL': entities.get('total', '')\n",
    "  }\n",
    "  return res_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xNzyUfUf9DIR"
   },
   "source": [
    "Now we will create a function creates a dataframe from the data in the `train` and `test` folders. The function will take the path to the `train` and `test` folders as input and return a dataframe with the columns discussed earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "D_Se1Txy9DIR"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def get_image_dimensions(img_path):\n",
    "  img = Image.open(img_path)\n",
    "  return img.size\n",
    "\n",
    "def create_df(data_dir):\n",
    "  box_path = os.path.join(data_dir, 'box')\n",
    "  img_path = os.path.join(data_dir, 'img')\n",
    "  entities_path = os.path.join(data_dir, 'entities')\n",
    "\n",
    "  image_path_list = []\n",
    "  bboxes_list = []\n",
    "  words_list = []\n",
    "  ner_tags_list = []\n",
    "\n",
    "  for file in os.listdir(img_path):\n",
    "    id = Path(file).stem\n",
    "\n",
    "    box_file_path = os.path.join(box_path, f'{id}.txt')\n",
    "    entities_file_path = os.path.join(entities_path, f'{id}.txt')\n",
    "\n",
    "    img_width, img_height = get_image_dimensions(os.path.join(img_path, file))\n",
    "\n",
    "    try:\n",
    "      with open(box_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        words = [get_word(line) for line in lines]\n",
    "        bboxes = [get_normalized_bbox(line, img_width, img_height) for line in lines]\n",
    "\n",
    "      entities = get_entities_dict(entities_file_path)\n",
    "      ner_tags = [get_word_tag(word, entities) for word in words]\n",
    "\n",
    "      image_path_list.append(os.path.join(img_path, file))\n",
    "      bboxes_list.append(bboxes)\n",
    "      words_list.append(words)\n",
    "      ner_tags_list.append(ner_tags)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "  df = pd.DataFrame({\n",
    "      'image_path': image_path_list,\n",
    "      'bboxes': bboxes_list,\n",
    "      'words': words_list,\n",
    "      'ner_tags': ner_tags_list\n",
    "  })\n",
    "\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(626, 345)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = create_df('data/unprocessed/SROIE2019/train')\n",
    "test_df = create_df('data/unprocessed/SROIE2019/test')\n",
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice that one test recipt has been dropped. This file contains Euro sign which isn't supported by utf-8. I decided to drop this image by adding the try except block around the code that reads the file. This will allow us to skip the file and continue with the rest of the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of our labelling stragey, we will calculate how many of each tag we have assigned to the words in the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>address</th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>not_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>626.000000</td>\n",
       "      <td>626.000000</td>\n",
       "      <td>626.000000</td>\n",
       "      <td>626.000000</td>\n",
       "      <td>626.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.107029</td>\n",
       "      <td>4.966454</td>\n",
       "      <td>1.305112</td>\n",
       "      <td>1.214058</td>\n",
       "      <td>10.592652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.065267</td>\n",
       "      <td>4.195673</td>\n",
       "      <td>1.103249</td>\n",
       "      <td>0.440575</td>\n",
       "      <td>4.888374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.750000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>33.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>39.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            total     address     company        date   not_other\n",
       "count  626.000000  626.000000  626.000000  626.000000  626.000000\n",
       "mean     3.107029    4.966454    1.305112    1.214058   10.592652\n",
       "std      2.065267    4.195673    1.103249    0.440575    4.888374\n",
       "min      1.000000    0.000000    0.000000    0.000000    2.000000\n",
       "25%      2.000000    3.000000    1.000000    1.000000    8.000000\n",
       "50%      3.000000    4.000000    1.000000    1.000000    9.000000\n",
       "75%      4.000000    5.750000    1.000000    1.000000   12.000000\n",
       "max     33.000000   25.000000    9.000000    3.000000   39.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_stats_df(df):\n",
    "  stats_not_other = df['ner_tags'].apply(lambda x: sum([1 for tag in x if tag != labels2ids['O']]))\n",
    "  stats_total = df['ner_tags'].apply(lambda x: sum([1 for tag in x if tag == labels2ids['B-TOTAL']]))\n",
    "  stats_address  = df['ner_tags'].apply(lambda x: sum([1 for tag in x if tag == labels2ids['B-ADDRESS']]))\n",
    "  stats_company = df['ner_tags'].apply(lambda x: sum([1 for tag in x if tag == labels2ids['B-COMPANY']]))\n",
    "  stats_date = df['ner_tags'].apply(lambda x: sum([1 for tag in x if tag == labels2ids['B-DATE']]))\n",
    "  stats_df = pd.DataFrame({\n",
    "      'total': stats_total,\n",
    "      'address': stats_address,\n",
    "      'company': stats_company,\n",
    "      'date': stats_date,\n",
    "      'not_other': stats_not_other\n",
    "  })\n",
    "  return stats_df\n",
    "\n",
    "stats_df = create_stats_df(train_df)\n",
    "stats_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results in the table, we find\n",
    "- The median number of bounding boxes assigned to `company` tag is 1. This implies the success of our labelling strategy.\n",
    "- The median number of bounding boxes assigned to `date` tag is 1. This also implies the success of our labelling strategy.\n",
    "- The median number of bounding boxes assigned to `address` tag is a whooping 4. The most likely reason for this is that the address is usually written in multiple lines and the bounding boxes may not cover the entire text. Moreover, the address may be written multiple times in the receipt.\n",
    "- The median number of bounding boxes assigned to `total` tag is 3. One possible explanation is that the total is usually written multiple times in the receipt. For example, if you buy only one item, the total value will be written once as item price and once as the total price. Other recipts may include total price after discount and total price before discount which are very likely to be the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want our model to take an image and return a json object with the following format:\n",
    "```json\n",
    "{\n",
    "    \"company\": \"COMPANY_NAME\",\n",
    "    \"date\": \"DATE\",\n",
    "    \"address\": \"ADDRESS\",\n",
    "    \"total\": \"TOTAL\",\n",
    "}\n",
    "```\n",
    "Our model may predict the same entity multiple times. Thus we need a stragey to pick only one bounding box for each entity. We will use the following stragey:\n",
    "- **Company**: We will pick the largest bounding box assigned to the company tag. This is because the company name is usually written with a larger font size than other text in the receipt.\n",
    "- **Date**: We will pick the bounding box assigned to the date tag that is closest to the top of the image. This is because the date is usually written at the top of the receipt.\n",
    "- **Address**: We will once again pick the largest bounding box assigned to the address tag. The LayoutLMv3 processor won't read text on seperate lines in the same box. Hence, it is unlikely for us to have one bounding box including the entire address. Our best bet is to pick the largest bounding box hoping it has the most important part of the address.\n",
    "- **Total**: We will pick the bounding box assigned to the total tag that is closest to the bottom of the image. This is because the total is usually written at the bottom of the receipt. Moreover, if some of the cases where the total is written multiple times occur, the total at the bottom is the most likely to be the final total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enhance the performance of our model, we will add this information the training and test datasets. We will create a function that takes the bounding boxes and the ner tags and recreates the tags such that at most one bounding box is assigned to each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recompute_ner_tags(row):\n",
    "  ner_tags = row['ner_tags']\n",
    "  bboxes = row['bboxes']\n",
    "  \n",
    "  new_ner_tags = [labels2ids['O'] for _ in ner_tags]\n",
    "  \n",
    "  # Give company tag only to the largest bounding box\n",
    "  company_idx = [i for i, tag in enumerate(ner_tags) if tag == labels2ids['B-COMPANY']]\n",
    "  if len(company_idx) > 0:\n",
    "    max_idx = max(company_idx, key=lambda x: (bboxes[x][2] - bboxes[x][0]) * (bboxes[x][3] - bboxes[x][1]))\n",
    "    new_ner_tags[max_idx] = labels2ids['B-COMPANY']\n",
    "    \n",
    "  # Give address tag only to the largest bounding box\n",
    "  address_idx = [i for i, tag in enumerate(ner_tags) if tag == labels2ids['B-ADDRESS']]\n",
    "  if len(address_idx) > 0:\n",
    "    max_idx = max(address_idx, key=lambda x: (bboxes[x][2] - bboxes[x][0]) * (bboxes[x][3] - bboxes[x][1]))\n",
    "    new_ner_tags[max_idx] = labels2ids['B-ADDRESS']\n",
    "    \n",
    "  # Give date tag to the earliest bounding box\n",
    "  date_idx = [i for i, tag in enumerate(ner_tags) if tag == labels2ids['B-DATE']]\n",
    "  if len(date_idx) > 0:\n",
    "    min_idx = min(date_idx, key=lambda x: (bboxes[x][1] + bboxes[x][3])/2)\n",
    "    new_ner_tags[min_idx] = labels2ids['B-DATE']\n",
    "  \n",
    "  # Give total tag to the last bounding box\n",
    "  total_idx = [i for i, tag in enumerate(ner_tags) if tag == labels2ids['B-TOTAL']]\n",
    "  if len(total_idx) > 0:\n",
    "    max_idx = max(total_idx, key=lambda x: (bboxes[x][1] + bboxes[x][3])/2)\n",
    "    new_ner_tags[max_idx] = labels2ids['B-TOTAL']\n",
    "    \n",
    "  return new_ner_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['ner_tags'] = train_df.apply(recompute_ner_tags, axis=1)\n",
    "test_df['ner_tags'] = test_df.apply(recompute_ner_tags, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>address</th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>not_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>626.0</td>\n",
       "      <td>626.000000</td>\n",
       "      <td>626.000000</td>\n",
       "      <td>626.000000</td>\n",
       "      <td>626.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.992013</td>\n",
       "      <td>0.880192</td>\n",
       "      <td>0.996805</td>\n",
       "      <td>3.869010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.089085</td>\n",
       "      <td>0.324997</td>\n",
       "      <td>0.056478</td>\n",
       "      <td>0.342365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       total     address     company        date   not_other\n",
       "count  626.0  626.000000  626.000000  626.000000  626.000000\n",
       "mean     1.0    0.992013    0.880192    0.996805    3.869010\n",
       "std      0.0    0.089085    0.324997    0.056478    0.342365\n",
       "min      1.0    0.000000    0.000000    0.000000    2.000000\n",
       "25%      1.0    1.000000    1.000000    1.000000    4.000000\n",
       "50%      1.0    1.000000    1.000000    1.000000    4.000000\n",
       "75%      1.0    1.000000    1.000000    1.000000    4.000000\n",
       "max      1.0    1.000000    1.000000    1.000000    4.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stats_df = create_stats_df(train_df)\n",
    "train_stats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>total</th>\n",
       "      <th>address</th>\n",
       "      <th>company</th>\n",
       "      <th>date</th>\n",
       "      <th>not_other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>345.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>345.000000</td>\n",
       "      <td>345.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.997101</td>\n",
       "      <td>0.985507</td>\n",
       "      <td>0.881159</td>\n",
       "      <td>0.991304</td>\n",
       "      <td>3.855072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.053838</td>\n",
       "      <td>0.119684</td>\n",
       "      <td>0.324071</td>\n",
       "      <td>0.092979</td>\n",
       "      <td>0.360691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            total     address     company        date   not_other\n",
       "count  345.000000  345.000000  345.000000  345.000000  345.000000\n",
       "mean     0.997101    0.985507    0.881159    0.991304    3.855072\n",
       "std      0.053838    0.119684    0.324071    0.092979    0.360691\n",
       "min      0.000000    0.000000    0.000000    0.000000    2.000000\n",
       "25%      1.000000    1.000000    1.000000    1.000000    4.000000\n",
       "50%      1.000000    1.000000    1.000000    1.000000    4.000000\n",
       "75%      1.000000    1.000000    1.000000    1.000000    4.000000\n",
       "max      1.000000    1.000000    1.000000    1.000000    4.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stats_df = create_stats_df(test_df)\n",
    "test_stats_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's much better. The table shows the median number of bounding boxes assigned to any tag is 1. This will make our model better able to understand the receipt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "\n",
    "I will start by directly converting the dataframes to huggingface datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['image_path', 'bboxes', 'words', 'ner_tags'],\n",
       "     num_rows: 626\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['image_path', 'bboxes', 'words', 'ner_tags'],\n",
       "     num_rows: 345\n",
       " }))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will instantiate the LayoutLMv3 processor to tokenize the words in the images, resize the images, and add padding to the tokenized words.\n",
    "\n",
    "We will add the argument `apply_ocr=False` to the processor to avoid applying OCR on the images. While fine-tuning the model, we will train the head of the model on the annotated data. If we apply OCR on the images, we won't have the labels for the extracted words. However, we will apply OCR on the images when we use the model for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ibrahim/anaconda3/envs/d2l/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "model_id = \"microsoft/layoutlmv3-base\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, apply_ocr=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create a function that maps the unprocessed data to the processed data. The function will take the unprocessed examples and use the processor to create the processed examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_examples(examples):\n",
    "  images = [Image.open(path).convert('RGB') for path in examples[\"image_path\"]]\n",
    "  words = examples[\"words\"]\n",
    "  bboxes = examples[\"bboxes\"]\n",
    "  ner_tags = examples[\"ner_tags\"]\n",
    "  encoding = processor(images, words, boxes=bboxes, word_labels=ner_tags, padding=\"max_length\", truncation=True)\n",
    "  return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we apply the mapping, we are going to define the features of the processed dataset.\n",
    "\n",
    "*Note that 224 x 224 is the image size expected by the LayoutLMv3 model and 512 is the maximum sequence length expected by the model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Features, Sequence, Value, Array2D, Array3D\n",
    "\n",
    "features = Features({\n",
    "  'pixel_values': Array3D(dtype=\"float32\", shape=(3, 224, 224)),\n",
    "  'input_ids': Sequence(feature=Value(dtype='int64')),\n",
    "  'attention_mask': Sequence(Value(dtype='int64')),\n",
    "  'bbox': Array2D(dtype=\"int64\", shape=(512, 4)),\n",
    "  'labels': Sequence(feature=Value(dtype='int64')),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will apply the mapping to the train and test datasets and save them to disk.\n",
    "\n",
    "Notice that I have changed the default `batch_size` to 32. The deafult batch size is 1000. Loading a 1000 image simultaneously will consume a lot of memory. I have reduced the batch size to 32 to avoid memory issues. You can increase the batch size if you have enough memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3acdac40e12f4eb2aedb60e1973120e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/626 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "298af4dff97b44e68e35c61c239a0a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(prepare_examples, batched=True, remove_columns=train_dataset.column_names, batch_size=32, features=features)\n",
    "test_dataset = test_dataset.map(prepare_examples, batched=True, remove_columns=test_dataset.column_names, batch_size=32, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cb4c017ed9c4a11b6624bbd9ee3c179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/626 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10d63d3105a34750b3e6ad2804871eed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/345 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset.save_to_disk('data/train')\n",
    "test_dataset.save_to_disk('data/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we have preprocessed the SROIE dataset to create a train and test dataset that can be consumed by the LayoutLMv3 model. We have created a dataframe with the necessary information and then processed the data to create a huggingface dataset. The processed dataset is saved to disk and can be used to fine-tune the LayoutLMv3 model. Now we are ready to the next step where we build the recipt understanding model."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
